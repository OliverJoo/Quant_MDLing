{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "import yfinance as yf\n",
    "\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense, GRU, Bidirectional,\n",
    "                                     Embedding, BatchNormalization, Dropout)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "def format_time(t):\n",
    "    m, s = divmod(t, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return f'{h:02.0f}:{m:02.0f}:{s:02.0f}'\n",
    "\n",
    "deciles = np.arange(.1, 1, .1).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get stock price data\n",
    "\n",
    "data_path = Path('data', 'sec-filings')\n",
    "results_path = Path('results', 'sec-filings')\n",
    "\n",
    "selected_section_path = results_path / 'ngrams_1'\n",
    "ngram_path = results_path / 'ngrams'\n",
    "vector_path = results_path / 'vectors'\n",
    "\n",
    "for path in [vector_path, selected_section_path, ngram_path]:\n",
    "    if not path.exists():\n",
    "        path.mkdir(parents=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get filing info\n",
    "filing_index = (pd.read_csv(data_path / 'filing_index.csv', parse_dates=['DATE_FILED'])\n",
    "                .rename(columns=str.lower))\n",
    "filing_index.index += 1\n",
    "filing_index.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filing_index.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filing_index.ticker.nunique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filing_index.date_filed.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Download stock price data using Yfinance\n",
    "yf_data, missing = [], []\n",
    "for i, (symbol, dates) in enumerate(filing_index.groupby('ticker').date_filed, 1):\n",
    "\n",
    "    if i % 250 == 0:\n",
    "        print(i, len(yf_data), len(set(missing)), flush=True)\n",
    "\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    for filing, date in dates.to_dict().items():\n",
    "        start = date - timedelta(days=93)\n",
    "        end = date + timedelta(days=31)\n",
    "        df = ticker.history(start=start, end=end)\n",
    "        if df.empty:\n",
    "            missing.append(symbol)\n",
    "        else:\n",
    "            yf_data.append(df.assign(ticker=symbol, filing=filing))\n",
    "\n",
    "yf_data = pd.concat(yf_data).rename(columns=str.lower)\n",
    "yf_data.to_hdf(results_path / 'sec_returns.h5', 'data/yfinance')\n",
    "yf_data = pd.read_hdf(results_path / 'sec_returns.h5', 'data/yfinance')\n",
    "yf_data.ticker.nunique()\n",
    "yf_data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get (some) missing prices from Quandl\n",
    "to_do = (filing_index.loc[~filing_index.ticker.isin(yf_data.ticker.unique()), ['ticker', 'date_filed']])\n",
    "to_do.date_filed.min()\n",
    "quandl_tickers = (pd.read_hdf('../data/assets.h5', 'quandl/wiki/prices').loc[idx['2012':, :], :]\n",
    "                  .index.unique('ticker'))\n",
    "quandl_tickers = list(set(quandl_tickers).intersection(set(to_do.ticker)))\n",
    "print(f'{len(quandl_tickers)}')\n",
    "\n",
    "to_do = filing_index.loc[filing_index.ticker.isin(quandl_tickers), ['ticker', 'date_filed']]\n",
    "to_do.info()\n",
    "\n",
    "ohlcv = ['adj_open', 'adj_high', 'adj_low', 'adj_close', 'adj_volume']\n",
    "quandl = (pd.read_hdf('data/assets.h5', 'quandl/wiki/prices').loc[idx['2012': , quandl_tickers], ohlcv]\n",
    "          .rename(columns=lambda x: x.replace('adj_', '')))\n",
    "\n",
    "quandl.info()\n",
    "quandl_data = []\n",
    "for i, (symbol, dates) in enumerate(to_do.groupby('ticker').date_filed, 1):\n",
    "    if i % 100 == 0:\n",
    "        print(i, end=' ', flush=True)\n",
    "    for filing, date in dates.to_dict().items():\n",
    "        start = date - timedelta(days=93)\n",
    "        end = date + timedelta(days=31)\n",
    "        quandl_data.append(quandl.loc[idx[start:end, symbol], :].reset_index('ticker').assign(filing=filing))\n",
    "\n",
    "quandl_data = pd.concat(quandl_data)\n",
    "quandl_data.to_hdf(results_path / 'sec_returns.h5', 'data/quandl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Combine, clean and persist\n",
    "data = (pd.read_hdf(results_path / 'sec_returns.h5', 'data/yfinance')\n",
    "        .drop(['dividends', 'stock splits'], axis=1)\n",
    "        .append(pd.read_hdf(results_path / 'sec_returns.h5', 'data/quandl')))\n",
    "\n",
    "data = data.loc[:, ['filing', 'ticker', 'open', 'high', 'low', 'close', 'volume']]\n",
    "data.info()\n",
    "data[['filing', 'ticker']].nunique()\n",
    "data.to_hdf(results_path / 'sec_returns.h5', 'prices')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Copy filings with stock price data\n",
    "data = pd.read_hdf(results_path / 'sec_returns.h5', 'prices')\n",
    "filings_with_data = data.filing.unique()\n",
    "len(filings_with_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove short and long sentences\n",
    "min_sentence_length = 5\n",
    "max_sentence_length = 50\n",
    "sent_length = Counter()\n",
    "for i, idx in enumerate(filings_with_data, 1):\n",
    "    if i % 500 == 0:\n",
    "        print(i, end=' ', flush=True)\n",
    "    text = pd.read_csv(data_path / 'selected_sections' / f'{idx}.csv').text\n",
    "    sent_length.update(text.str.split().str.len().tolist())\n",
    "    text = text[text.str.split().str.len().between(min_sentence_length, max_sentence_length)]\n",
    "    text = '\\n'.join(text.tolist())\n",
    "    with (selected_section_path / f'{idx}.txt').open('w') as f:\n",
    "        f.write(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sent_length = pd.Series(dict(sent_length.most_common()))\n",
    "with sns.axes_style(\"white\"):\n",
    "    sent_length.sort_index().cumsum().div(sent_length.sum()).loc[5:51].plot.bar(figsize=(12, 4), rot=0)\n",
    "    sns.despine();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    sent_length.sort_index().loc[:50].plot.bar(figsize=(14, 4))\n",
    "    sns.despine();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create bi- and trigrams\n",
    "files = selected_section_path.glob('*.txt')\n",
    "texts = [f.read_text() for f in files]\n",
    "unigrams = ngram_path / 'ngrams_1.txt'\n",
    "unigrams.write_text('\\n'.join(texts))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "texts = unigrams.read_text()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_grams = []\n",
    "start = time()\n",
    "for i, n in enumerate([2, 3]):\n",
    "    sentences = LineSentence(ngram_path / f'ngrams_{n-1}.txt')\n",
    "    phrases = Phrases(sentences=sentences,\n",
    "                      min_count=25,  # ignore terms with a lower count\n",
    "                      threshold=0.5,  # accept phrases with higher score\n",
    "                      max_vocab_size=4000000,  # prune of less common words to limit memory use\n",
    "                      delimiter=b'_',  # how to join ngram tokens\n",
    "                      scoring='npmi')\n",
    "\n",
    "    s = pd.DataFrame([[k.decode('utf-8'), v] for k, v in phrases.export_phrases(sentences)],\n",
    "                     columns=['phrase', 'score']).assign(length=n)\n",
    "\n",
    "    n_grams.append(s.groupby('phrase').score.agg(['mean', 'size']))\n",
    "    print(n_grams[-1].nlargest(5, columns='size'))\n",
    "\n",
    "    grams = Phraser(phrases)\n",
    "    sentences = grams[sentences]\n",
    "    (ngram_path / f'ngrams_{n}.txt').write_text('\\n'.join([' '.join(s) for s in sentences]))\n",
    "\n",
    "    src_dir = results_path / f'ngrams_{n-1}'\n",
    "    target_dir = results_path / f'ngrams_{n}'\n",
    "    if not target_dir.exists():\n",
    "        target_dir.mkdir()\n",
    "\n",
    "    for f in src_dir.glob('*.txt'):\n",
    "        text = LineSentence(f)\n",
    "        text = grams[text]\n",
    "        (target_dir / f'{f.stem}.txt').write_text('\\n'.join([' '.join(s) for s in text]))\n",
    "    print('\\n\\tDuration: ', format_time(time() - start))\n",
    "\n",
    "n_grams = pd.concat(n_grams).sort_values('size', ascending=False)\n",
    "n_grams.to_parquet(results_path / 'ngrams.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_grams.groupby(n_grams.index.str.replace('_', ' ').str.count(' ')).size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert filings to integer sequences based on token count\n",
    "sentences = (ngram_path / 'ngrams_3.txt').read_text().split('\\n')\n",
    "n = len(sentences)\n",
    "\n",
    "token_cnt = Counter()\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    if i % 500000 == 0:\n",
    "        print(f'{i/n:.1%}', end=' ', flush=True)\n",
    "    token_cnt.update(sentence.split())\n",
    "\n",
    "token_cnt = pd.Series(dict(token_cnt.most_common()))\n",
    "token_cnt = token_cnt.reset_index()\n",
    "token_cnt.columns = ['token', 'n']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "token_cnt.to_parquet(results_path / 'token_cnt')\n",
    "token_cnt.n.describe(deciles).apply(lambda x: f'{x:,.0f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "token_cnt.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "token_cnt.nlargest(10, columns='n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "token_cnt.sort_values(by=['n', 'token'], ascending=[False, True]).head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "token_by_freq = token_cnt.sort_values(by=['n', 'token'], ascending=[False, True]).token\n",
    "token2id = {token: i for i, token in enumerate(token_by_freq, 3)}\n",
    "len(token2id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for token, i in token2id.items():\n",
    "    print(token, i)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_sequences(min_len=100, max_len=20000, num_words=25000, oov_char=2):\n",
    "    if not vector_path.exists():\n",
    "        vector_path.mkdir()\n",
    "    seq_length = {}\n",
    "    skipped = 0\n",
    "    for i, f in tqdm(enumerate((results_path / 'ngrams_3').glob('*.txt'), 1)):\n",
    "        file_id = f.stem\n",
    "        text = f.read_text().split('\\n')\n",
    "        vector = [token2id[token] if token2id[token] + 2 < num_words else oov_char for line in text\n",
    "                  for token in line.split()]\n",
    "        vector = vector[:max_len]\n",
    "        if len(vector) < min_len:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        seq_length[int(file_id)] = len(vector)\n",
    "        np.save(vector_path / f'{file_id}.npy', np.array(vector))\n",
    "    seq_length = pd.Series(seq_length)\n",
    "    return seq_length\n",
    "\n",
    "seq_length = generate_sequences()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.Series(seq_length).to_csv(results_path / 'seq_length.csv')\n",
    "seq_length.describe(deciles)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seq_length.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(18,5))\n",
    "token_cnt.n.plot(logy=True, logx=True, ax=axes[0], title='Token Frequency (log-log scale)')\n",
    "sent_length.sort_index().loc[:50].plot.bar(ax=axes[1], rot=0, title='Sentence Length')\n",
    "\n",
    "n=5\n",
    "ticks = axes[1].xaxis.get_ticklocs()\n",
    "ticklabels = [l.get_text() for l in axes[1].xaxis.get_ticklabels()]\n",
    "axes[1].xaxis.set_ticks(ticks[n-1::n])\n",
    "axes[1].xaxis.set_ticklabels(ticklabels[n-1::n])\n",
    "axes[1].set_xlabel('Sentence Length')\n",
    "\n",
    "sns.distplot(seq_length, ax=axes[2], bins=50)\n",
    "axes[0].set_ylabel('Token Frequency')\n",
    "axes[0].set_xlabel('Token ID')\n",
    "\n",
    "axes[2].set_xlabel('# Words per Filing')\n",
    "axes[2].set_title('Filing Length Distribution')\n",
    "\n",
    "fig.suptitle('Corpus Stats', fontsize=13)\n",
    "sns.despine()\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=.85)\n",
    "fig.savefig(results_path / 'sec_seq_len', dpi=300);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "files = vector_path.glob('*.npy')\n",
    "filings = sorted([int(f.stem) for f in files])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare Model Data\n",
    "\n",
    "# Create weekly forward returns\n",
    "prices = pd.read_hdf(results_path / 'sec_returns.h5', 'prices')\n",
    "prices.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fwd_return = {}\n",
    "for filing in filings:\n",
    "    date_filed = filing_index.at[filing, 'date_filed']\n",
    "    price_data = prices[prices.filing==filing].close.sort_index()\n",
    "\n",
    "    try:\n",
    "        r = (price_data.pct_change(periods=5).shift(-5).loc[:date_filed].iloc[-1])\n",
    "    except:\n",
    "        continue\n",
    "    if not np.isnan(r) and -.5 < r < 1:\n",
    "        fwd_return[filing] = r\n",
    "len(fwd_return)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Combine returns with filing data\n",
    "y, X = [], []\n",
    "for filing_id, fwd_ret in fwd_return.items():\n",
    "    X.append(np.load(vector_path / f'{filing_id}.npy') + 2)\n",
    "    y.append(fwd_ret)\n",
    "\n",
    "y = np.array(y)\n",
    "len(y), len(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "maxlen = 20000\n",
    "X_train = pad_sequences(X_train, truncating='pre', padding='pre', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, truncating='pre', padding='pre', maxlen=maxlen)\n",
    "X_train.shape, X_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define Model Architecture\n",
    "K.clear_session()\n",
    "\n",
    "embedding_size = 100\n",
    "input_dim = X_train.max() + 1\n",
    "rnn = Sequential([\n",
    "    Embedding(input_dim=input_dim, output_dim=embedding_size, input_length=maxlen, name='EMB'),\n",
    "    BatchNormalization(name='BN1'),\n",
    "    Bidirectional(GRU(32), name='BD1'),\n",
    "    BatchNormalization(name='BN2'),\n",
    "    Dropout(.1, name='DO1'),\n",
    "    Dense(5, name='D'),\n",
    "    Dense(1, activation='linear', name='OUT')\n",
    "])\n",
    "\n",
    "rnn.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rnn.compile(loss='mse', optimizer='Adam', metrics=[RootMeanSquaredError(name='RMSE'),\n",
    "                     MeanAbsoluteError(name='MAE')])\n",
    "\n",
    "# Train model\n",
    "early_stopping = EarlyStopping(monitor='val_MAE', patience=5, restore_best_weights=True)\n",
    "\n",
    "training = rnn.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test),\n",
    "                   callbacks=[early_stopping], verbose=1, workers=6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the Results\n",
    "df = pd.DataFrame(training.history)\n",
    "df.to_csv(results_path / 'rnn_sec.csv', index=False)\n",
    "df.index += 1\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(16, 8), sharey=True)\n",
    "plot_data = (df[['RMSE', 'val_RMSE']].rename(columns={'RMSE': 'Training', 'val_RMSE': 'Validation'}))\n",
    "plot_data.plot(ax=axes[0], title='Root Mean Squared Error')\n",
    "\n",
    "plot_data = (df[['MAE', 'val_MAE']].rename(columns={'MAE': 'Training', 'val_MAE': 'Validation'}))\n",
    "plot_data.plot(ax=axes[1], title='Mean Absolute Error')\n",
    "\n",
    "for i in [0, 1]:\n",
    "    axes[i].set_xlim(1, 10)\n",
    "    axes[i].set_xlabel('Epoch')\n",
    "fig.tight_layout()\n",
    "fig.savefig(results_path / 'sec_cv_performance', dpi=300);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_score = rnn.predict(X_test)\n",
    "rho, p = spearmanr(y_score.squeeze(), y_test)\n",
    "print(f'Information Coefficient: {rho*100:.2f} ({p:.2%})')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g = sns.jointplot(y_score.squeeze(), y_test, kind='reg');"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}